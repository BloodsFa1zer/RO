# Лабораторна робота №2

**Тема:** Паралельне множення матриць. Алгоритм Фокса (MPI)  
**Дисципліна:** Розподілені обчислення

---

## 1. Мета роботи

1. Ознайомитися з паралельними алгоритмами множення матриць.
2. Реалізувати послідовний алгоритм множення двох квадратних матриць `n × n`.
3. Реалізувати паралельний алгоритм множення матриць за схемою **Фокса** з використанням MPI.
4. Дослідити час виконання послідовної та паралельної реалізацій для різних розмірів матриць та різної кількості процесів.
5. Оцінити прискорення та ефективність паралельної реалізації.

---

## 2. Постановка задачі

Дано дві квадратні матриці розміру `n × n`:

- матриця \( A = (a_{ij}) \),
- матриця \( B = (b_{ij}) \),

потрібно обчислити матрицю

\[
C = A \cdot B,
\]

де

\[
c_{ij} = \sum_{k=0}^{n-1} a_{ik} \cdot b_{kj}, \quad i,j = 0, \dots, n-1.
\]

Потрібно:

- реалізувати послідовний алгоритм обчислення матриці \(C\);
- реалізувати паралельний алгоритм Фокса з шаховим (checkerboard) розподілом матриць між процесами MPI;
- порівняти час виконання та розрахувати прискорення.

---

## 3. Організація проєкту та середовище

### 3.1. Структура каталогу

Проєкт організовано так:

```text
Lab2/
  serial/
    main.cpp      # послідовне множення матриць
  mpi/
    main.cpp      # паралельна реалізація (алгоритм Фокса, MPI)
  serial_mm       # скомпільована послідовна програма
  mpi_mm          # скомпільована паралельна програма
  README.md       # цей файл
```

### 3.2. WSL / macOS / Linux

Робота ведеться у Linux-середовищі (WSL / Linux / macOS з інструментами GNU):

- компілятор: `g++`;
- MPI-реалізація: OpenMPI (або еквівалент).

Приклад встановлення (Ubuntu / WSL):

```bash
sudo apt update
sudo apt install -y g++ openmpi-bin libopenmpi-dev
```

Приклад для macOS (через Homebrew):

```bash
brew install open-mpi
```

## 4. Послідовна реалізація (serial/main.cpp)

### 4.1. Ініціалізація даних

У послідовній програмі реалізовано:

- введення розміру `n` з консолі з перевіркою `n > 0`;
- виділення пам'яті під матриці `A`, `B`, `C` як векторів розміру `n * n`;
- ініціалізація:

```cpp
A[i * n + j] = static_cast<double>(i + j + 1);
B[i * n + j] = (i == j) ? 1.0 : 0.0;  // майже одинична матриця
C[i * n + j] = 0.0;
```

Таке задання дозволяє просто контролювати коректність: добуток з майже одиничною матрицею не «ламати» структуру `A`.

### 4.2. Алгоритм множення

Множення реалізовано трьома вкладеними циклами:

```cpp
for (i = 0; i < n; ++i) {
    for (j = 0; j < n; ++j) {
        double sum = 0.0;
        for (k = 0; k < n; ++k) {
            sum += A[i * n + k] * B[k * n + j];
        }
        C[i * n + j] = sum;
    }
}
```

### 4.3. Вимірювання часу

Час до та після множення фіксується через `clock()` з `<ctime>`. Обчислюється `T_serial` у секундах:

\[
T_{serial} = \frac{clock_{finish} - clock_{start}}{CLOCKS\_PER\_SEC}
\]

Для малих `n` (наприклад, `n ≤ 5`) матриця `C` виводиться на екран для візуальної перевірки.

---

## 5. Паралельна реалізація (mpi/main.cpp, алгоритм Фокса)

### 5.1. Основні ідеї алгоритму Фокса

Всі `p` процесів організуються в квадратну сітку `q × q`, де `p = q²`.

Матриці `A`, `B`, `C` розбиваються на блоки розміру `blockSize × blockSize`:

```text
n % q == 0, blockSize = n / q
```

Кожен процес відповідає за один блок `A_local`, `B_local`, `C_local`.

На кожному з `q` етапів:

1. У кожному рядку вибирається «ведучий» блок `A` і розсилається (`MPI_Bcast`) усім процесам у цьому рядку.
2. Кожен процес множить отриманий блок `A` на свій локальний блок `B` і додає результат до `C_local`.
3. Блоки `B` циклічно зсуваються вздовж стовпців (`MPI_Cart_shift` + `MPI_Sendrecv_replace`).

Після `q` етапів кожен процес має свою частину результату `C_local`.

### 5.2. Обмеження та перевірки

У `main` перед запуском алгоритму перевіряється:

1. Кількість процесів `world_size` повинна бути повним квадратом:

```cpp
int q = (int)std::sqrt((double)world_size);
if (q * q != world_size) {
    // Number of processes must be a perfect square (q*q).
}
```

2. Розмір матриці `n` має ділитися на `q`:

```cpp
if (n % q != 0) {
    // Matrix size n must be divisible by q.
}
```

Якщо умови не виконано — програма виводить повідомлення і завершує виконання.

### 5.3. Топологія процесів та комунікатори

Створюється 2D декартова топологія:

```cpp
int dims[2]    = { q, q };
int periods[2] = { 1, 0 };  // циклічність по рядках, без циклу по стовпцях
MPI_Comm cart_comm;
MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 1, &cart_comm);
```

Кожен процес отримує свої координати `(row, col)`:

```cpp
int coords[2];
MPI_Cart_coords(cart_comm, world_rank, 2, coords);
int row = coords[0];
int col = coords[1];
```

Для кожного рядка створюється `row_comm` через `MPI_Comm_split`, що використовується для `MPI_Bcast` блоків `A`.

### 5.4. Розподіл даних

На процесі `rank 0` ініціалізуються глобальні матриці `A_global`, `B_global`.

Функції `distribute_matrix` / `collect_matrix`:

- розрізають глобальні матриці на блоки `blockSize × blockSize`;
- розсилають їх усім процесам;
- збирають назад результуючу матрицю `C_global` з наборів `C_local`.

### 5.5. Основний цикл Фокса

Для `stage` від `0` до `q - 1`:

```cpp
int rootCol = (row + stage) % q;  // індекс стовпця, який розсилає блок A

if (col == rootCol) {
    A_temp = A_local;             // процес з col == rootCol готує блок для розсилки
}

MPI_Bcast(&A_temp[0], blockSize * blockSize, MPI_DOUBLE,
          rootCol, row_comm);     // розсилка A_temp по рядку

block_multiply_add(A_temp, B_local, C_local, blockSize); // C_local += A_temp * B_local

// Циклічний зсув B вздовж стовпців сітки
int src, dst;
MPI_Cart_shift(cart_comm, 0, -1, &src, &dst);
MPI_Sendrecv_replace(&B_local[0], blockSize * blockSize, MPI_DOUBLE,
                     dst, 2, src, 2, cart_comm, &status);
```

### 5.6. Вимірювання часу та перевірка результату

До/після основного циклу Фокса використовується `MPI_Wtime()`:

```cpp
MPI_Barrier(cart_comm);
double t_start = MPI_Wtime();
// ... Fox ...
MPI_Barrier(cart_comm);
double t_finish = MPI_Wtime();
double local_time = t_finish - t_start;
MPI_Reduce(&local_time, &max_time, 1, MPI_DOUBLE, MPI_MAX, 0, cart_comm);
```

На процесі `rank 0`:

- `max_time` використовується як час паралельного виконання `T_parallel`;
- виконується послідовне множення `serial_multiply(A_global, B_global, C_serial, n)`;
- результати `C_global` і `C_serial` порівнюються з допуском `1e-6`.

При збігу виводиться:

```
=== Parallel result matches serial computation ===
```

---

## 6. Компіляція та запуск

### 6.1. Компіляція

```bash
# Перехід у каталог Lab2
cd Lab2

# Послідовна версія
g++ -std=c++17 -O2 serial/main.cpp -o serial_mm

# Паралельна версія (Fox, MPI)
mpic++ -std=c++17 -O2 mpi/main.cpp -o mpi_mm
```

### 6.2. Запуск

```bash
# Послідовна програма
./serial_mm

# Паралельна програма (приклад запусків)
mpirun -np 4  ./mpi_mm    # q = 2, p = 4
mpirun -np 9  ./mpi_mm    # q = 3, p = 9
mpirun -np 16 ./mpi_mm    # q = 4, p = 16
```

**Умова:**

- кількість процесів `p` має бути квадратом цілого числа `q` (`p = q²`);
- розмір `n` має ділитися на `q`.

**Примітка:** На деяких системах для запуску більшої кількості процесів може знадобитися використати опцію `--oversubscribe`:

```bash
mpirun --oversubscribe -np 16 ./mpi_mm
```

---

## 7. Експериментальні результати

Нижче — узагальнююча таблиця, заповнена за результатами запусків. Використано значення `n`, що діляться на `q` (або близькі до них, якщо точне ділення неможливе).

### 7.1. Таблиця вимірювань

**Позначення:**

- `n` — розмір матриць;
- `p` — кількість процесів (`p = q²`);
- `T_serial` — час послідовної програми (сек);
- `T_parallel` — час паралельної програми (сек);
- `S_p = T_serial / T_parallel` — прискорення;
- `E_p = S_p / p` — ефективність.

| n | p | T_serial (s) | T_parallel (s) | S_p = T_serial / T_parallel | E_p = S_p / p |
|---|---|--------------|----------------|------------------------------|---------------|
| 100 | 4 | 0.003308 | 0.001579 | 2.095 | 0.524 |
| 100 | 9 | 0.002497 | 0.001457 | 1.714 | 0.190 |
| 100 | 16 | 0.003308 | 0.004937 | 0.670 | 0.042 |
| 500 | 4 | 0.267263 | 0.092144 | 2.902 | 0.726 |
| 500 | 9 | 0.262418 | 0.077000 | 3.408 | 0.379 |
| 500 | 16 | 0.267263 | 0.104531 | 2.558 | 0.160 |
| 1000 | 4 | 2.16145 | 0.729267 | 2.963 | 0.741 |
| 1000 | 9 | 2.10835 | 0.512340 | 4.111 | 0.457 |
| 1000 | 16 | 2.16145 | 0.436488 | 4.951 | 0.309 |

**Примітки:**

- Для `p = 9` використано найближчі значення `n`, що діляться на `q = 3`: `n = 102` (замість 100), `n = 498` (замість 500), `n = 1002` (замість 1000). У таблиці вказано значення `n = 100, 500, 1000` для зручності порівняння, але фактичні вимірювання виконано для вказаних вище значень.
- Для `p = 16` використано опцію `--oversubscribe` через обмеження кількості слотів на системі.

### 7.2. Аналіз результатів

1. **Для малих матриць (n ≈ 100):**
   - При `p = 4` досягнуто прискорення `S_p = 2.095` з ефективністю `E_p = 0.524`.
   - При `p = 9` прискорення `S_p = 1.714` з ефективністю `E_p = 0.190` — накладні витрати на комунікацію значно впливають на продуктивність.
   - При `p = 16` спостерігається зниження продуктивності (`S_p = 0.670`) через домінування накладних витрат над обчислювальним навантаженням.

2. **Для середніх матриць (n ≈ 500):**
   - При `p = 4` досягнуто хороше прискорення `S_p = 2.902` з високою ефективністю `E_p = 0.726`.
   - При `p = 9` прискорення зростає до `S_p = 3.408` з ефективністю `E_p = 0.379`.
   - При `p = 16` прискорення `S_p = 2.558` з ефективністю `E_p = 0.160` — збільшення кількості процесів не покращує продуктивність через комунікаційні витрати.

3. **Для великих матриць (n ≈ 1000):**
   - При `p = 4` досягнуто прискорення `S_p = 2.963` з високою ефективністю `E_p = 0.741`.
   - При `p = 9` прискорення значно зростає до `S_p = 4.111` з ефективністю `E_p = 0.457`.
   - При `p = 16` досягнуто найкраще прискорення `S_p = 4.951`, хоча ефективність знижується до `E_p = 0.309` через збільшення кількості процесів.

**Висновок:** Для великих матриць паралельний алгоритм Фокса демонструє значне прискорення. Ефективність найкраща при `p = 4` для всіх розмірів матриць, що вказує на оптимальний баланс між обчислювальним навантаженням та комунікаційними витратами. Для великих матриць (`n ≈ 1000`) використання більшої кількості процесів (`p = 9, 16`) дає краще абсолютне прискорення, хоча ефективність знижується.

---

## 8. Висновки

У ході виконання лабораторної роботи:

1. **Реалізовано послідовний алгоритм** множення квадратних матриць `n × n` та проведено вимірювання часу виконання.

2. **Реалізовано паралельний алгоритм Фокса** за допомогою MPI:
   - процеси організовано у 2D-декартову топологію `q × q`;
   - матриці розподілено на блоки `blockSize × blockSize` між процесами;
   - реалізовано розсилку блоків матриці `A` по рядках та циклічний зсув блоків `B` по стовпцях.

3. **Паралельний результат було перевірено** шляхом порівняння з послідовним множенням (відхилення не перевищують заданого допуску `1e-6`).

4. **На основі отриманих експериментальних даних** (часи `T_serial` і `T_parallel`) обчислено прискорення `S_p` та ефективність `E_p`.

5. **Для малих `n`** прискорення невисоке через домінування накладних витрат MPI (створення процесів, комунікації). Для більших `n` частка обчислень зростає, і паралельний алгоритм дає помітне прискорення порівняно з послідовною реалізацією.

6. **Отримані результати підтверджують доцільність** використання розподілених обчислень для задач множення великих матриць і демонструють типову залежність прискорення та ефективності від розміру задачі та числа процесів.
