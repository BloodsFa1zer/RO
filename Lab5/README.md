# Лабораторна робота №5: Паралельні алгоритми обробки графів (Алгоритм Флойда)

**Дисципліна:** Розподілені обчислення

**Тема:** Паралельні алгоритми обробки графів (Алгоритм Флойда-Воршелла)

---

## Мета роботи

Розробка та дослідження ефективності паралельної версії алгоритму Флойда-Воршелла для пошуку найкоротших шляхів між усіма парами вершин у зваженому графі. Аналіз прискорення обчислень при використанні технології MPI порівняно з послідовною версією.

---

## Опис реалізації

### 1. Послідовний алгоритм

Базується на класичному алгоритмі Флойда-Воршелла.

- **Структура даних:** Матриця суміжності розміром $N \times N$, розгорнута в одновимірний масив.

- **Логіка:** Три вкладені цикли перебирають усі пари вершин $(i, j)$ через проміжну вершину $k$. На кожній ітерації $k$ алгоритм перевіряє, чи можна покращити найкоротший шлях від $i$ до $j$, використовуючи вершину $k$ як проміжну.

- **Складність:** $O(N^3)$.

### 2. Паралельний алгоритм (MPI)

Використовується **стрічкова схема розбиття даних (1D decomposition)** по рядках.

**Ключові етапи:**

1. **Декомпозиція даних:**

   - Головний процес (Rank 0) генерує матрицю.

   - Використовується `MPI_Scatterv` для розподілу рядків матриці між процесами. Це дозволяє обробляти ситуації, коли $N$ не ділиться на кількість процесів націло (використовуються масиви зміщень `Displs` та лічильників `SendCounts`).

2. **Основний цикл:**

   - На кожній ітерації $k$ (де $k$ — проміжна вершина) процес, що володіє $k$-м рядком, розсилає його всім іншим процесам за допомогою `MPI_Bcast`.

   - Кожен процес оновлює лише свою частину рядків, використовуючи отриманий $k$-й рядок. Це мінімізує обсяг комунікацій порівняно з пересилкою всієї матриці.

3. **Збір результатів:**

   - Після завершення обчислень оновлені смуги рядків збираються на головному процесі за допомогою `MPI_Gatherv`.

---

## Структура проєкту

```
Lab5/
  ├─ SerialFloyd.cpp      # Послідовна реалізація алгоритму Флойда
  ├─ ParallelFloyd.cpp    # Паралельна реалізація алгоритму Флойда (MPI)
  └─ README.md            # Цей файл
```

---

## Компіляція та запуск

### Попередня підготовка

Переконайтеся, що встановлено компілятор `g++` та бібліотеку MPI (наприклад, OpenMPI).

**Встановлення на Ubuntu/WSL:**
```bash
sudo apt update
sudo apt install -y g++ openmpi-bin libopenmpi-dev
```

**Встановлення на macOS (через Homebrew):**
```bash
brew install open-mpi
```

### Компіляція

**Послідовна версія:**
```bash
cd Lab5
g++ -O2 SerialFloyd.cpp -o SerialFloyd
```

**Паралельна версія:**
```bash
mpic++ -O2 ParallelFloyd.cpp -o ParallelFloyd
```

### Запуск

**Послідовна:**
```bash
./SerialFloyd
```

**Паралельна (наприклад, на 4 процесорах):**
```bash
mpirun -n 4 ./ParallelFloyd
```

**Примітка:** На деяких системах для запуску більшої кількості процесів може знадобитися використати опцію `--oversubscribe`:
```bash
mpirun --oversubscribe -n 8 ./ParallelFloyd
```

### Автоматичне тестування

Для автоматичного запуску тестів з різними розмірами графів використовується скрипт `run_tests.sh`:

```bash
./run_tests.sh
```

Скрипт автоматично:
- Компілює обидві версії програми
- Запускає тести для розмірів: 10, 500, 600, 700, 800, 900, 1000 вершин
- Тестує паралельну версію на 2, 4 та 8 процесах
- Зберігає результати у файли: `serial_results.txt`, `parallel_2_results.txt`, `parallel_4_results.txt`, `parallel_8_results.txt`

Для обробки результатів та обчислення прискорення та теоретичних значень:

```bash
python3 process_results.py
```

**Режим quiet:** Обидві програми підтримують режим `quiet`, який виводить тільки розмір та час виконання у форматі `розмір час`:
```bash
./SerialFloyd 1000 quiet
mpirun -n 4 ./ParallelFloyd 1000 quiet
```

---

## Результати експериментів

Експерименти проводилися на графі з кількістю вершин від 10 до 1000. Вимірювався час виконання послідовної версії та паралельної на 2, 4 та 8 процесах.

### Таблиця 1. Часові показники та прискорення (Speedup)

| Кількість вершин | Serial (сек) | 2 Proc (сек) | Speedup (2) | 4 Proc (сек) | Speedup (4) | 8 Proc (сек) | Speedup (8) |
|------------------|--------------|--------------|-------------|--------------|-------------|--------------|-------------|
| **10** | 0.000002     | 0.000021     | 0.10        | 0.000089     | 0.02        | 0.000116     | 0.02        |
| **500** | 0.097544     | 0.045017     | 2.17        | 0.026139     | 3.73        | 0.031815     | 3.07        |
| **600** | 0.156580     | 0.076029     | 2.06        | 0.041704     | 3.75        | 0.056124     | 2.79        |
| **700** | 0.250505     | 0.119464     | 2.10        | 0.065194     | 3.84        | 0.081276     | 3.08        |
| **800** | 0.375020     | 0.174491     | 2.15        | 0.101027     | 3.71        | 0.245631     | 1.53        |
| **900** | 0.526592     | 0.247689     | 2.13        | 0.137941     | 3.82        | 0.185356     | 2.84        |
| **1000** | 0.720403     | 0.339092     | 2.12        | 0.182550     | 3.95        | 0.247588     | 2.91        |

---

## Порівняння з теоретичною моделлю

### Таблиця 2. Порівняння експериментального та теоретичного часу послідовного алгоритму

Розрахунок теоретичного часу базується на формулі $T = Size^3 \cdot \tau$.

За опорне значення для розрахунку $\tau$ взято експеримент із **800 вершинами**.

**Обчислене значення $\tau$:** $7.32461 \cdot 10^{-10}$ сек.

| Test Number | Vertices | Execution Time (sec) | Theoretical Time (sec) |
|-------------|----------|----------------------|------------------------|
| 1           | 10       | 0.000002             | 0.000001               |
| 2           | 500      | 0.097544             | 0.091558               |
| 3           | 600      | 0.156580             | 0.158212               |
| 4           | 700      | 0.250505             | 0.251234               |
| 5           | 800      | 0.375020             | 0.375020               |
| 6           | 900      | 0.526592             | 0.533964               |
| 7           | 1000     | 0.720403             | 0.732461               |

### Порівняння експериментального та теоретичного часу паралельного алгоритму

Для оцінки ефективності використано модель ідеального лінійного прискорення:

$$T_{model} = \frac{T_{serial}}{P}$$

де $P$ — кількість процесів.

### Таблиця 3. Порівняння експериментального та теоретичного часу

| Вершини | 2 Proc (Model) | 2 Proc (Exp) | 4 Proc (Model) | 4 Proc (Exp) | 8 Proc (Model) | 8 Proc (Exp) |
|---------|----------------|--------------|----------------|--------------|----------------|--------------|
| **10** | 0.000001       | 0.000021     | 0.000000       | 0.000089     | 0.000000       | 0.000116     |
| **500** | 0.048772       | 0.045017     | 0.024386       | 0.026139     | 0.012193       | 0.031815     |
| **800** | 0.187510       | 0.174491     | 0.093755       | 0.101027     | 0.046878       | 0.245631     |
| **1000**| 0.360202       | 0.339092     | 0.180101       | 0.182550     | 0.090050       | 0.247588     |

---

## Висновки

1. **Малі графи (10 вершин):** Спостерігається "сповільнення" (Speedup < 1). Це очікувана поведінка, оскільки накладні витрати на ініціалізацію MPI та комунікацію (пересилку даних мережею/пам'яттю) значно перевищують час корисних обчислень, який для 10 вершин є мізерним.

2. **Великі графи (500-1000 вершин):** Ефективність паралелізації різко зростає.

   - Для **1000 вершин** прискорення на 4 процесорах становить **3.95**, що дуже близько до ідеального значення (4.0).

   - На 2 процесорах прискорення становить близько **2.1-2.2**, що також демонструє високу ефективність.

   - Це пояснюється тим, що обчислювальна складність зростає як $O(N^3)$, а комунікаційна складність (передача рядків) зростає повільніше — як $O(N^2)$. Тобто з ростом $N$ частка корисних обчислень збільшується відносно часу комунікацій.

3. **Відхилення від моделі:** Експериментальний час трохи більший за теоретичний на 4 та 8 процесах через витрати на синхронізацію (`MPI_Bcast`) та нерівномірність доступу до пам'яті, проте результати демонструють високу ефективність розробленого алгоритму для задач великої розмірності.
