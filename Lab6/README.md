# Лабораторна робота №6  

**Тема:** Паралельні алгоритми розв'язання диференціальних рівнянь у частинних похідних  

(задача Діріхле для рівняння Пуассона, метод Гаусса–Зейделя)

---

## Мета роботи

1. Реалізувати послідовний алгоритм Гаусса–Зейделя для задачі Діріхле для рівняння Пуассона.

2. Розробити паралельний варіант алгоритму з блочно-смужковою декомпозицією за допомогою MPI.

3. Дослідити час виконання серійної та паралельної програм, порівняти з теоретичною оцінкою.

---

## Теоретичні відомості (коротко)

Розглядається задача Діріхле для рівняння Пуассона в одиничному квадраті $D = \{(x,y): 0 \le x, y \le 1\}$:

$$\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = f(x,y), \quad (x,y) \in D$$

$$u(x,y)\big|_{\partial D} = g(x,y)$$

Область дискретизується рівномірною сіткою з кроком $h = 1/(N+1)$, де $N$ – кількість внутрішніх вузлів по кожній координаті.

Для п'яточкового шаблону маємо різницеве рівняння:

$$u_{ij} = \frac{1}{4} \left(u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1} - h^2 f_{ij}\right)$$

У лабораторній роботі функцію правої частини беруть $f(x,y) \equiv 0$, а граничну умову – $g(x,y) \equiv 100$, тобто на всій границі $u = 100$, а всередині початкове наближення $u_{ij}^{(0)} = 0$.

Ітераційна формула методу Гаусса–Зейделя:

$$u_{ij}^{(k+1)} = \frac{1}{4} \left( u_{i-1,j}^{(k+1)} + u_{i+1,j}^{(k)} + u_{i,j-1}^{(k+1)} + u_{i,j+1}^{(k)} \right)$$

де на новому кроці використовуються вже оновлені значення «зверху» і «зліва».

Ітерації повторюються, поки максимальна зміна

$$d_{\max} = \max_{i,j} |u_{ij}^{(k+1)} - u_{ij}^{(k)}|$$

не стане меншою за задану точність $\varepsilon$.

Теоретична оцінка часу роботи серійного алгоритму:

$$T_i = \tau \cdot k \cdot m \cdot N^2$$

де:
- $N$ – кількість внутрішніх вузлів по одній координаті
- $m = 6$ – число операцій на один вузол
- $k$ – кількість ітерацій до досягнення точності
- $\tau$ – час однієї базової операції

---

## Опис реалізації

### 1. Послідовна програма `SerialGS.cpp`

Основні функції:

- `ProcessInitialization` – введення розміру сітки `Size > 2`, точності `Eps > 0`, виділення пам'яті та ініціалізація граничних умов: `u = 100` на границі, `u = 0` всередині.   

- `DummyDataInitialization` – задає граничні умови.  

- `PrintMatrix` – форматований вивід матриці (використовується лише для невеликих `Size`).  

- `ResultCalculation` – реалізація методу Гаусса–Зейделя згідно з псевдокодом методички.  

- `ProcessTermination` – звільнення пам'яті.

Для заміру часу використовується `clock()`.

### 2. Паралельна програма `ParallelGS.cpp`

Паралельний варіант виконує ті ж самі обчислення, але матриця розбивається на горизонтальні смуги (block-striped decomposition).

Основні ідеї:

- `Size x Size` – повна матриця на процесі 0.

- Внутрішні рядки (без верхнього й нижнього граничних) розбиваються на `ProcNum` однакових блоків по `(RowNum-2)` рядків.  

- Кожен процес зберігає `RowNum` рядків:  

  - 0-й – граничний або «привид» згори,  

  - `1 .. RowNum-2` – власні внутрішні рядки,  

  - `RowNum-1` – граничний або «привид» знизу.

Важливі функції:

- `ProcessInitialization` –  

  - процес 0 вводить `Size`, `Eps` і перевіряє, що `Size > 2`, `Size ≥ ProcNum`, `(Size-2) % ProcNum == 0`;  

  - за допомогою `MPI_Bcast` розсилає параметри всім процесам;  

  - виділяє пам'ять для `pMatrix` (тільки на 0-му процесі) та `pProcRows` (на всіх).

- `DataDistribution` – розподіл внутрішніх рядків за допомогою `MPI_Scatter` + відправка нижнього граничного рядка на останній процес.  

- `ExchangeData` – обмін граничними рядками між сусідніми процесами через `MPI_Sendrecv` (спочатку вниз, потім вгору).  

- `IterationCalculation` – локальна ітерація ГЗ по смузі процесу, повертає локальний максимум відхилення.  

- `ParallelResultCalculation` – повний паралельний цикл:  

  1. `ExchangeData` – обмін граничними рядками,  

  2. `IterationCalculation` – локальна ітерація,  

  3. `MPI_Allreduce` з операцією `MPI_MAX` для обчислення глобального `Delta`,  

  4. повтор, поки `Delta > Eps`.  

- `ResultCollection` – збирання внутрішніх рядків у повну матрицю на процесі 0 за допомогою `MPI_Gather`.

---

## Компіляція та запуск (macOS, arm64)

### Послідовна версія

```bash
g++ -O2 SerialGS.cpp -o SerialGaussSeidel
./SerialGaussSeidel
```

Програма попросить ввести:
- Розмір сітки (Size > 2)
- Точність (Eps > 0)

### Паралельна версія

**Встановлення MPI (якщо ще не встановлено):**
```bash
brew install open-mpi
```

**Компіляція:**
```bash
mpic++ -O2 ParallelGS.cpp -o ParallelGaussSeidel
```

**Запуск:**
```bash
# На 2 процесах
mpirun -n 2 ./ParallelGaussSeidel

# На 4 процесах
mpirun -n 4 ./ParallelGaussSeidel

# На 8 процесах (може знадобитися --oversubscribe)
mpirun --oversubscribe -n 8 ./ParallelGaussSeidel
```

**Важливо:** Для паралельної версії необхідно, щоб:
- `Size >= ProcNum`
- `(Size - 2) % ProcNum == 0` (кількість внутрішніх рядків повинна ділитися на кількість процесів)

**Приклади валідних значень:**
- Size = 10, ProcNum = 2: (10-2) % 2 = 0 ✓
- Size = 12, ProcNum = 4: (12-2) % 4 = 2 ✗
- Size = 14, ProcNum = 4: (14-2) % 4 = 0 ✓

---

## Структура проєкту

```
Lab6/
  ├─ SerialGS.cpp      # Послідовна реалізація методу Гаусса–Зейделя
  ├─ ParallelGS.cpp    # Паралельна реалізація (MPI)
  ├─ README.md         # Цей файл
  ├─ test.sh           # Bash скрипт для автоматичного тестування
  └─ TestGS.cpp        # C++ тести для детальної перевірки
```

---

## Результати експериментів

### Таблиця 1. Часові показники та прискорення

| Size | Eps | Serial (сек) | 2 Proc (сек) | Speedup (2) | 4 Proc (сек) | Speedup (4) | 8 Proc (сек) | Speedup (8) |
|------|-----|--------------|--------------|-------------|--------------|-------------|--------------|-------------|
| 10   | 0.01| -            | -            | -           | -            | -           | -            | -           |
| 50   | 0.01| -            | -            | -           | -            | -           | -            | -           |
| 100  | 0.01| -            | -            | -           | -            | -           | -            | -           |
| 200  | 0.01| -            | -            | -           | -            | -           | -            | -           |

*Примітка: Заповніть таблицю після проведення експериментів*

### Таблиця 2. Кількість ітерацій

| Size | Eps | Serial Iter | 2 Proc Iter | 4 Proc Iter | 8 Proc Iter |
|------|-----|-------------|-------------|-------------|-------------|
| 10   | 0.01| -           | -           | -           | -           |
| 50   | 0.01| -           | -           | -           | -           |
| 100  | 0.01| -           | -           | -           | -           |

*Примітка: Кількість ітерацій повинна бути однаковою для послідовної та паралельної версій*

---

## Порівняння з теоретичною моделлю

### Теоретична оцінка часу послідовного алгоритму

$$T_{serial} = \tau \cdot k \cdot m \cdot N^2$$

де:
- $N = Size - 2$ (кількість внутрішніх вузлів)
- $m = 6$ (операції на вузол)
- $k$ – кількість ітерацій
- $\tau$ – час однієї операції (визначається експериментально)

### Теоретична оцінка часу паралельного алгоритму

$$T_{parallel} = \frac{T_{serial}}{P} + T_{comm}$$

де:
- $P$ – кількість процесів
- $T_{comm}$ – час комунікацій (обмін граничними рядками)

### Таблиця 3. Порівняння експериментального та теоретичного часу

| Size | Serial (Exp) | Serial (Theor) | 4 Proc (Exp) | 4 Proc (Theor) |
|------|--------------|----------------|--------------|----------------|
| 100  | -            | -              | -            | -              |
| 200  | -            | -              | -            | -              |

*Примітка: Заповніть після проведення експериментів та обчислення $\tau$*

---

## Висновки

1. **Коректність:** Паралельна версія повинна давати ті ж самі результати, що й послідовна (з урахуванням точності).

2. **Ефективність:** Для великих розмірів сітки (Size > 100) паралелізація дає значне прискорення.

3. **Масштабованість:** Зі збільшенням кількості процесів прискорення зростає, але накладні витрати на комунікацію також збільшуються.

4. **Обмеження:** Для малих розмірів сітки (Size < 50) накладні витрати MPI можуть перевищувати виграш від паралелізації.

---

## Тестування

### Bash скрипт (test.sh)

Для автоматичного тестування використовуйте скрипт `test.sh`:

```bash
chmod +x test.sh
./test.sh
```

Скрипт перевіряє:
- Коректність компіляції обох програм
- Роботу на різних розмірах сітки (10, 50, 100)
- Порівняння результатів послідовної та паралельної версій
- Різні значення точності (0.1, 0.01, 0.001)
- Валідацію вводу (перевірка на некоректні значення)

### C++ тести (TestGS.cpp)

Для більш детального тестування використовуйте C++ тестовий файл:

```bash
# Спочатку скомпілюйте основні програми
g++ -O2 SerialGS.cpp -o SerialGaussSeidel
mpic++ -O2 ParallelGS.cpp -o ParallelGaussSeidel

# Компілюємо та запускаємо тести
g++ -O2 TestGS.cpp -o TestGS
./TestGS
```

C++ тести перевіряють:
- Граничні умови (всі граничні значення повинні бути 100.0)
- Порівняння числових результатів послідовної та паралельної версій
- Збіжність алгоритму для різних значень точності
- Порівняння матриць результатів (для малих розмірів)
