# Лабораторна робота №4: Паралельні методи сортування даних

**Дисципліна:** Розподілені обчислення  

**Тема:** Паралельні методи сортування даних (Bubble Sort / Odd-Even Transposition Sort)

-----

## Опис завдання та Мета роботи

Метою лабораторної роботи є практичне вивчення принципів розробки паралельних алгоритмів для обробки великих масивів даних. Основним завданням є реалізація та дослідження ефективності алгоритму сортування методом бульбашки (**Bubble Sort**) та його паралельної модифікації — **Odd-Even Transposition Sort** (сортування парним-непарним переставлянням) — із використанням бібліотеки **MPI (Message Passing Interface)**.

В ході роботи необхідно:

1. Реалізувати послідовний алгоритм Bubble Sort для оцінки базової продуктивності.

2. Використати стандартний алгоритм `std::sort` як еталон швидкодії ($O(n \log n)$).

3. Розробити паралельний додаток, що розподіляє дані між процесами, сортує їх локально та об'єднує результати.

4. Провести порівняльний аналіз часу виконання та прискорення (Speedup) на різній кількості процесорів.

-----

## Структура проєкту

Проєкт розділений на два незалежні модулі:

* **`SerialBubbleSort.cpp`** — Послідовна реалізація.

    * Генерує тестовий набір даних.

    * Виконує класичне сортування бульбашкою.

    * Виконує стандартне сортування (`std::sort`) для порівняння.

    * Заміряє час виконання обох алгоритмів.

* **`SerialBubbleSortTest.cpp`** — Допоміжні функції для послідовної версії (вивід масиву, обгортки сортування).

* **`SerialBubbleSortTest.h`** — Заголовочний файл з деклараціями функцій.

* **`ParallelBubbleSort.cpp`** — Паралельна реалізація (MPI).

    * Ініціалізує MPI-середовище.

    * Розподіляє дані між процесами (`Scatterv`).

    * Реалізує алгоритм Odd-Even Transposition.

    * Збирає відсортовані дані (`Gatherv`).

* **`ParallelBubbleSortTest.cpp`** — Допоміжні функції для паралельної версії (перевірка коректності, копіювання даних).

* **`ParallelBubbleSortTest.h`** — Заголовочний файл з деклараціями функцій.

* **`ParallelBubbleSort.h`** — Заголовочний файл з деклараціями функцій паралельної версії.

-----

## Опис алгоритмів

### 1. Послідовний алгоритм (Bubble Sort)

Класичний алгоритм сортування обміном.

* **Принцип:** Прохід по масиву та обмін сусідніх елементів, якщо вони стоять у неправильному порядку.

* **Складність:** $O(n^2)$.

* **Недолік:** Дуже повільний на великих обсягах даних, проте ідеальний для демонстрації ефективності розпаралелювання квадратичних алгоритмів.

### 2. Паралельний алгоритм (Odd-Even Transposition)

Це паралельна модифікація бульбашкового сортування, адаптована для систем з розподіленою пам'яттю.

**Етапи виконання:**

1. **Ініціалізація та Розподіл:**

    * Масив розбивається на блоки. Оскільки $N$ може не ділитися на $P$ (кількість процесорів), реалізовано механізм обробки блоків різного розміру за допомогою `MPI_Scatterv`.

2. **Локальне сортування:**

    * Кожен процес спочатку сортує свій отриманий блок послідовним методом.

3. **Комунікаційні ітерації (Odd-Even):**

    * Виконується $P$ ітерацій.

    * На **непарних** кроках: процеси з непарним рангом ($1, 3, \dots$) обмінюються даними з сусідами справа ($2, 4, \dots$).

    * На **парних** кроках: процеси з парним рангом ($0, 2, \dots$) обмінюються з сусідами справа ($1, 3, \dots$).

    * **Операція Compare-Split:** Процеси обмінюються блоками, зливають їх в один відсортований масив, а потім ділять навпіл: "молодший" процес залишає собі менші значення, "старший" — більші.

4. **Збір даних:**

    * Використовується `MPI_Gatherv` для збору відсортованих частин у фінальний масив на кореневому процесі.

**Використані функції MPI:**

* `MPI_Init` / `MPI_Finalize`

* `MPI_Comm_size` / `MPI_Comm_rank`

* `MPI_Bcast` (розсилка $N$)

* `MPI_Scatterv` / `MPI_Gatherv` (робота з нерівними блоками)

* `MPI_Sendrecv` (обмін даними без блокування)

-----

## Компіляція та Запуск

### Попередня підготовка

Переконайтеся, що встановлено компілятор `g++` та бібліотеку MPI (наприклад, OpenMPI).

**Встановлення на Ubuntu/WSL:**
```bash
sudo apt update
sudo apt install -y g++ openmpi-bin libopenmpi-dev
```

**Встановлення на macOS (через Homebrew):**
```bash
brew install open-mpi
```

### Компіляція

**Послідовна версія:**

```bash
cd Lab4
g++ SerialBubbleSort.cpp SerialBubbleSortTest.cpp -o SerialBubbleSort
```

**Паралельна версія:**

```bash
mpic++ ParallelBubbleSort.cpp ParallelBubbleSortTest.cpp -o ParallelBubbleSort
```

### Запуск

**Послідовна:**

```bash
./SerialBubbleSort
```

Програма автоматично тестує різні розміри масивів та виводить таблицю з результатами.

**Паралельна (наприклад, на 4 процесорах):**

```bash
mpirun -n 4 ./ParallelBubbleSort
```

Програма автоматично тестує різні розміри масивів для заданої кількості процесів.

**Примітка:** На деяких системах для запуску більшої кількості процесів може знадобитися використати опцію `--oversubscribe`:

```bash
mpirun --oversubscribe -n 8 ./ParallelBubbleSort
```

-----

## Результати експериментів

Експерименти проводилися на масивах від 10 до 50 000 елементів.

### Таблиця 4.1. Час виконання послідовних алгоритмів

| № тесту | Обсяг даних | Serial Bubble Sorting (сек) | Serial Standard Sorting (сек) |
|:-------:|:-----------:|:---------------------------:|:-----------------------------:|
| 1       | 10          | 0.000004                    | 0.000001                      |
| 2       | 100         | 0.000032                    | 0.000003                      |
| 3       | 10,000      | 0.111875                    | 0.000368                      |
| 4       | 20,000      | 0.369241                    | 0.000793                      |
| 5       | 30,000      | 0.909397                    | 0.001194                      |
| 6       | 40,000      | 1.829062                    | 0.001658                      |
| 7       | 50,000      | 3.124957                    | 0.002128                      |

### Таблиця 4.4. Результати паралельного сортування

| № тесту | Обсяг даних | Serial Bubble | 2 Процесори | 4 Процесори | 8 Процесорів |
|:-------:|:-----------:|:-------------:|:-----------:|:-----------:|:------------:|
| 1       | 10          | 0.000004      | 0.001453    | 0.000068    | 0.000168     |
| 2       | 100         | 0.000032      | 0.000021    | 0.000007    | 0.004777     |
| 3       | 10,000      | 0.111875      | 0.029902    | 0.013648    | 0.003561     |
| 4       | 20,000      | 0.369241      | 0.105691    | 0.033142    | 0.015882     |
| 5       | 30,000      | 0.909397      | 0.248816    | 0.095035    | 0.022337     |
| 6       | 40,000      | 1.829062      | 0.484302    | 0.145918    | 0.043025     |
| 7       | 50,000      | 3.124957      | 0.898190    | 0.220900    | 0.068205     |

### Таблиця 4.5. Прискорення (Speedup)

Розраховано відносно послідовного Bubble Sort ($SpeedUp = T_{serial} / T_{parallel}$).

| № | 2 Процесори | 4 Процесори | 8 Процесорів |
|:-:|:-----------:|:-----------:|:------------:|
| 1 | 0.003       | 0.059       | 0.024        |
| 2 | 1.524       | 4.571       | 0.007        |
| 3 | 3.741       | 8.199       | 31.424       |
| 4 | 3.493       | 11.137      | 23.265       |
| 5 | 3.653       | 9.571       | 40.707       |
| 6 | 3.777       | 12.535      | 42.535       |
| 7 | 3.483       | 14.153      | 45.821       |

### Таблиця 4.6. Порівняння з теоретичною моделлю

Використано модель ідеального лінійного прискорення: $T_{model} = T_{serial} / P$.

| № | Data Size | 2 Processors | 4 Processors | 8 Processors |
|:-:|:---------:|:------------:|:------------:|:------------:|
|   |           | **Model** \| **Experiment** | **Model** \| **Experiment** | **Model** \| **Experiment** |
| 1 | 10        | 0.000002 \| 0.001453 | 0.000001 \| 0.000068 | 0.0000005 \| 0.000168 |
| 2 | 100       | 0.000016 \| 0.000021 | 0.000008 \| 0.000007 | 0.000004 \| 0.004777 |
| 3 | 10,000    | 0.055938 \| 0.029902 | 0.027969 \| 0.013648 | 0.013984 \| 0.003561 |
| 4 | 20,000    | 0.184621 \| 0.105691 | 0.092310 \| 0.033142 | 0.046155 \| 0.015882 |
| 5 | 30,000    | 0.454699 \| 0.248816 | 0.227349 \| 0.095035 | 0.113675 \| 0.022337 |
| 6 | 40,000    | 0.914531 \| 0.484302 | 0.457266 \| 0.145918 | 0.228633 \| 0.043025 |
| 7 | 50,000    | 1.562479 \| 0.898190 | 0.781239 \| 0.220900 | 0.390620 \| 0.068205 |

## Висновки та аналіз результатів

### Аналіз результатів

1. **Проблема малих даних:** На малих масивах (10-100 елементів) паралельний алгоритм працює повільніше за послідовний (Speedup < 1) або показує незначне прискорення. Це зумовлено тим, що час, витрачений на ініціалізацію обмінів повідомленнями (latency), значно перевищує час корисних обчислень.

2. **Надлінійне прискорення (Super-linear Speedup):** Починаючи з 10 000 елементів, спостерігається значне зростання продуктивності. Наприклад, для 50 000 елементів на 8 процесорах прискорення становить **45.821x** (при теоретичному максимумі 8x).

    * **Причина:** Ефект кешування (Cache Effect). При розбитті великого масиву на менші блоки, кожен блок повністю поміщається у швидку кеш-пам'ять (L1/L2) відповідного процесора. Послідовний алгоритм працює з усім масивом в повільній оперативній пам'яті (RAM), що призводить до частих промахів кешу (cache misses).

3. **Порівняння зі std::sort:** Незважаючи на успішне розпаралелювання, стандартний алгоритм `std::sort` все одно швидший, оскільки має кращу асимптотичну складність ($O(n \log n)$ проти паралельного $O(n^2)$).

4. **Масштабованість:** Зі збільшенням кількості процесорів прискорення зростає для великих масивів. Наприклад, для 50 000 елементів:
   * 2 процесори: прискорення 3.483x
   * 4 процесори: прискорення 14.153x
   * 8 процесорів: прискорення 45.821x

5. **Ефективність паралелізації:** Для великих масивів (30 000+ елементів) паралельний алгоритм демонструє високу ефективність, значно перевищуючи теоретичне лінійне прискорення завдяки ефекту кешування.

### Висновки

У ході виконання лабораторної роботи:

1. **Реалізовано послідовний алгоритм** Bubble Sort та проведено порівняння зі стандартним `std::sort`, що підтвердило значну перевагу останнього завдяки кращій асимптотичній складності.

2. **Реалізовано паралельний алгоритм** Odd-Even Transposition Sort з використанням MPI:
   * дані розподіляються між процесами за допомогою `MPI_Scatterv`;
   * виконується локальне сортування на кожному процесі;
   * реалізовано алгоритм Odd-Even Transposition з операцією Compare-Split;
   * результати збираються за допомогою `MPI_Gatherv`.

3. **Проведено експериментальні вимірювання** для різних розмірів масивів (від 10 до 50 000 елементів) та різної кількості процесорів (2, 4, 8).

4. **Виявлено надлінійне прискорення** для великих масивів, що пояснюється ефектом кешування при розподілі даних між процесами.

5. **Отримані результати підтверджують доцільність** використання паралельних алгоритмів для сортування великих обсягів даних, навіть для алгоритмів з квадратичною складністю, якщо вони дозволяють ефективно використовувати кеш-пам'ять.
