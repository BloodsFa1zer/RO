# Лабораторна робота №3

**Тема:** Паралельні методи розв'язання систем лінійних рівнянь (метод Гауса)  
**Дисципліна:** Розподілені обчислення

---

## 1. Мета роботи

1. Ознайомитись з послідовною реалізацією методу Гауса для розв'язання систем лінійних алгебраїчних рівнянь (СЛАР).
2. Реалізувати паралельний варіант методу Гауса з використанням MPI та горизонтального розподілу рядків матриці між процесами.
3. Дослідити часові характеристики послідовного та паралельного алгоритмів, обчислити прискорення.
4. Порівняти експериментальні результати з теоретичними оцінками часу виконання для серійного та паралельного алгоритмів (формули (3.5) та (3.6) з методички).

---

## 2. Умова та короткий опис задачі

Розглядається задача розв'язання СЛАР:

$$A x = b$$

де:
- A ∈ R^(n×n) — квадратна матриця коефіцієнтів,
- b ∈ R^n — вектор правих частин,
- x ∈ R^n — вектор невідомих.

Метод Гауса складається з двох основних етапів:

1. **Прямий хід (Gaussian elimination)** — перетворення системи до верхньотрикутного вигляду шляхом виключення змінних.
2. **Зворотний хід (back substitution)** — послідовне обчислення невідомих, починаючи з останнього.

У лабораторній роботі необхідно:
- реалізувати **послідовний алгоритм методу Гауса**;
- реалізувати **паралельний алгоритм** методу Гауса з використанням MPI та розподілом рядків матриці між процесами;
- провести експерименти для різних розмірів системи та різної кількості процесів;
- заповнити таблиці з часом виконання, прискоренням та теоретичними оцінками.

---

## 3. Вимоги до середовища та організації роботи

- Робота виконується **під WSL / Linux-середовищем**.
- Усі кроки фіксуються в Git (GitHub / Bitbucket):
  - **кожна лабораторна робота — в окремій гілці** (`lab03-gauss` або подібна);
  - дрібні, логічно завершені коміти з осмисленими повідомленнями.
- Опис результатів і експериментів — у цьому файлі **`README.md`** у папці лабораторної роботи.

---

## 4. Структура проєкту

```text
Lab3/
  ├─ serial_gauss/
  │   └─ main.cpp        # Послідовна реалізація методу Гауса
  ├─ parallel_gauss_mpi/
  │   └─ main.cpp        # Паралельна реалізація методу Гауса (MPI)
  └─ README.md           # цей файл
```

---

## 5. Послідовний алгоритм методу Гауса

### 5.1. Ініціалізація даних

Використовуються дві схеми ініціалізації:

**DummyDataInitialization** — «штучні» прості дані для контролю коректності:
- A — нижня трикутна матриця з одиницями на діагоналі, елементи над діагоналлю дорівнюють 0;
- b вибирається так, щоб розв'язок був вектор із одиниць (це дозволяє легко перевірити правильність алгоритму).

**RandomDataInitialization** — випадкові значення для вимірювання часу:
- Матриця A — нижньотрикутна, заповнена випадковими числами з діагоналлю, відмінною від нуля;
- Вектор b — випадкові значення.

У коді це реалізується окремими функціями й викликається з `ProcessInitialization()`.

### 5.2. Прямий хід (Gaussian elimination)

Для кожної ітерації `Iter` (від 0 до `Size-1`):

1. Знайти опорний рядок (`FindPivotRow`) — рядок з максимальним за модулем елементом у стовпці `Iter` серед тих, що ще не використані як опорні.
2. Виконати елімінацію стовпця нижче опорного рядка (функція `SerialColumnElimination`) — занулити елементи під опорним, оновлюючи також вектор b.

Результат — матриця стає верхньотрикутною, вектор \(b\) змінений відповідно.

### 5.3. Зворотний хід (back substitution)

Починаючи з останнього рівняння (остання змінна), обчислюються значення невідомих:

$$x_i = \frac{b_i - \sum_{j=i+1}^{n-1} a_{ij} x_j}{a_{ii}}$$

Результат зберігається у векторі `pResult`.

Методичка вимагає окрему функцію для зворотного ходу (back substitution) та виклик її з `SerialResultCalculation()`.

### 5.4. Вимірювання часу послідовного алгоритму

Для вимірювання часу використовується `clock()`:

```cpp
time_t start, finish;
double duration;

start = clock();
SerialResultCalculation(pMatrix, pVector, pResult, Size);
finish = clock();
duration = (finish - start) / double(CLOCKS_PER_SEC);
```

Цей час заноситься до таблиці експериментів (див. розділ 8).

---

## 6. Паралельний алгоритм методу Гауса (MPI)

### 6.1. Схема розподілу даних

Використовується рядковий (строковий) розподіл матриці та вектора між процесами:

- Процес з рангом 0 зберігає повну матрицю A та вектор b.
- Рядки розподіляються між процесами приблизно рівномірно:
  - для цього використовуються масиви `pProcNum` (кількість рядків на процесі) та `pProcInd` (початковий індекс рядка кожного процесу);
  - розподіл реалізовано за допомогою `MPI_Scatterv` для матриці й вектора.

### 6.2. Паралельний прямий хід

На кожній ітерації:

1. Визначення, на якому процесі знаходиться опорний рядок (за глобальним номером рядка і масивом `pProcInd`).
2. Процес, що володіє опорним рядком:
   - нормує цей рядок;
   - готує його до розсилки.
3. Опорний рядок розсилається усім процесам за допомогою `MPI_Bcast`.
4. Кожен процес виконує елімінацію у своїх локальних рядках (оновлює `pProcRows` і `pProcVector`).

### 6.3. Паралельний зворотний хід

Для кожної невідомої (від останньої до першої):

1. Визначається процес, на якому знаходиться відповідний опорний рядок.
2. Цей процес обчислює поточну невідому `IterResult`.
3. Значення розсилається усім процесам (`MPI_Bcast`).
4. Кожен процес оновлює свій локальний вектор правих частин.

Локальні частини розв'язку зводяться у глобальний вектор (через `MPI_Gatherv` або заповнення `pResult` на кореневому процесі).

### 6.4. Вимірювання часу паралельного алгоритму

Згідно з методичкою, час вимірюється для всього паралельного етапу: розподіл даних + обчислення + збирання результату.

```cpp
double Start, Finish, Duration;

Start = MPI_Wtime();

// розподіл даних
DataDistribution(pMatrix, pProcRows, pVector, pProcVector, Size, RowNum);

// паралельний метод Гауса
ParallelResultCalculation(pProcRows, pProcVector, pProcResult, Size, RowNum);

// збирання результату
ResultCollection(pProcResult, pResult);

Finish = MPI_Wtime();
Duration = Finish - Start;

if (ProcRank == 0) {
    printf("\nTime of execution: %f\n", Duration);
}
```

---

## 7. Теоретичні оцінки часу виконання

### 7.1. Серійний алгоритм

Теоретичний час виконання серійного алгоритму оцінюється формулою (3.5):

$$T_1(n) = \left(\frac{2n^3}{3} + n^2\right) \tau$$

де:
- n — розмір системи,
- τ (tau) — час виконання однієї базової операції.

**Як знайти τ:**

1. Обрати один із експериментів (наприклад, з найбільшим n) як опорний.
2. Взяти експериментальний час T₁^exp(n_pivot).
3. Обчислити:

$$\tau = \frac{T_1^{exp}(n_{pivot})}{\frac{2n_{pivot}^3}{3} + n_{pivot}^2}$$

Після цього можна порахувати теоретичні значення T₁^theor(n) для всіх розмірів.

### 7.2. Паралельний алгоритм

Теоретичний час паралельного алгоритму T_p(n) береться з формули (3.6) в методичці:

$$T_p(n) = (\text{обчислення}) \cdot \tau + (\text{кількість повідомлень}) \cdot \alpha + (\text{обсяг даних}) \cdot \beta$$

де:
- p — кількість процесів,
- τ — як вище,
- α (alpha) — латентність мережі,
- β (beta) — параметр, що описує пропускну здатність мережі.

**Важливо:** Конкретний вигляд суми та множників для α і β береться безпосередньо з формули (3.6) в PDF (розділ 9 методички). У цьому README ти просто підставляєш уже відомі дані в готову формулу.

---

## 8. Експериментальна частина

Нижче — шаблони таблиць, які треба заповнити за результатами запусків.

### 8.1. Обчислення часу для серійного алгоритму

Рекомендовані розміри системи:

n ∈ {10, 100, 500, 1000, 1500, 2000, 2500, 3000}

**Таблиця 1** — Серійний метод Гауса: експеримент vs теорія (формула (3.5))

| № тесту | Розмір системи n | Час експериментальний T₁^exp, c | Теоретичний час T₁^theor, c | Відносна похибка |
|---------|---------------------|--------------------------------------|-----------------------------------|------------------------------------------------------------------|
| 1 | 10 | 5.00e-06 | 8.75e-08 | 98.25% |
| 2 | 100 | 8.10e-05 | 7.73e-05 | 4.63% |
| 3 | 500 | 0.008930 | 0.009542 | 6.86% |
| 4 | 1000 | 0.065978 | 0.076224 | 15.53% |
| 5 | 1500 | 0.213564 | 0.257127 | 20.40% |
| 6 | 2000 | 0.513551 | 0.609334 | 18.65% |
| 7 | 2500 | 1.066200 | 1.189928 | 11.60% |
| 8 | 3000 | 2.055990 | 2.055990 | 0.00% |

*Опорне значення: n = 3000, τ = 1.14e-10*

### 8.2. Паралельний метод Гауса (MPI) — час та прискорення

Рекомендовано запускати паралельну програму з кількістю процесів p ∈ {2, 4, 8} (якщо дозволяє середовище).

**Таблиця 2** — Паралельний метод Гауса: час, прискорення та теоретичні оцінки

| № тесту | Розмір системи n | Час серійний T₁^exp, c | p = 2: час T₂^exp, c | p = 2: прискорення S₂ | p = 2: теор. час T₂^theor, c | p = 4: час T₄^exp, c | p = 4: S₄ | p = 4: T₄^theor | p = 8: час T₈^exp, c | p = 8: S₈ | p = 8: T₈^theor |
|---------|---------------------|------------------------------|------------------------------|----------------------------------------|-------------------------------------|------------------------------|----------------|------------------------|------------------------------|----------------|----------------------|
| 1 | 10 | 5.00e-06 | 0.001188 | 0.00 | - | 0.001599 | 0.00 | - | 0.000135 | 0.04 | - |
| 2 | 100 | 8.10e-05 | 0.001429 | 0.06 | - | 0.002794 | 0.03 | - | 0.005689 | 0.01 | - |
| 3 | 500 | 0.008930 | 0.005988 | 1.49 | - | 0.009051 | 0.99 | - | 0.008640 | 1.03 | - |
| 4 | 1000 | 0.065978 | 0.047616 | 1.39 | - | 0.040411 | 1.63 | - | 0.052943 | 1.25 | - |
| 5 | 1500 | 0.213564 | 0.164035 | 1.30 | - | 0.100032 | 2.13 | - | 0.127759 | 1.67 | - |
| 6 | 2000 | 0.513551 | 0.435434 | 1.18 | - | 0.296228 | 1.73 | - | 0.362247 | 1.42 | - |
| 7 | 2500 | 1.066200 | 0.931320 | 1.14 | - | 0.727085 | 1.47 | - | 0.785820 | 1.36 | - |
| 8 | 3000 | 2.055990 | 1.713530 | 1.20 | - | 1.438570 | 1.43 | - | 1.411940 | 1.46 | - |

*Примітка: Теоретичні значення часу для паралельного алгоритму (колонки T_p^theor) обчислюються за формулою (3.6) з методички, яка потребує значень параметрів α та β для конкретної системи.*

---

## 9. Компіляція та запуск

### 9.1. Послідовна версія

```bash
cd Lab3/serial_gauss

g++ main.cpp -O2 -o serial_gauss

# Запуск
./serial_gauss
```

### 9.2. Паралельна версія (MPI)

```bash
cd Lab3/parallel_gauss_mpi

mpic++ main.cpp -O2 -o parallel_gauss

# Запуск на 2 процесах
mpirun -np 2 ./parallel_gauss

# Запуск на 4 процесах
mpirun -np 4 ./parallel_gauss

# Запуск на 8 процесах
mpirun -np 8 ./parallel_gauss
```